---
sidebar_position: 19
title: "Sensor Integration Workflow Tutorial"
---

# Sensor Integration Workflow Tutorial

## Overview

This tutorial demonstrates how to integrate various sensors into a Digital Twin simulation environment, focusing on the workflow for creating perception-ready robots. You'll learn to set up different sensor types, synchronize data streams, and validate sensor performance in simulation.

## Prerequisites

Before starting this tutorial, you should:
- Have completed the Basic Gazebo Simulation tutorial
- Understand fundamental robotics concepts (frames, transforms, coordinate systems)
- Be familiar with ROS message types for sensors
- Have access to basic robot models for testing

## Sensor Integration Fundamentals

### Understanding Sensor Types

Different sensors serve different purposes in robotics:

#### Range Sensors
- **LiDAR**: 3D mapping, navigation, obstacle detection
- **Depth Cameras**: 3D scene understanding, manipulation
- **Sonar/IR**: Proximity detection, simple obstacle avoidance
- **Stereo Cameras**: 3D reconstruction, depth estimation

#### Vision Sensors
- **RGB Cameras**: Object recognition, navigation, human interaction
- **Thermal Cameras**: Environmental monitoring, detection in low light
- **Event Cameras**: High-speed motion detection, low-latency sensing
- **Multi-spectral**: Specialized material identification

#### Inertial Sensors
- **IMU**: Balance, orientation, motion detection
- **Gyroscopes**: Angular velocity measurement
- **Accelerometers**: Linear acceleration measurement
- **Magnetometers**: Magnetic field and heading detection

#### Tactile Sensors
- **Force/Torque**: Grasping, manipulation, contact detection
- **Tactile Arrays**: Surface texture, object identification
- **Pressure Sensors**: Contact force measurement
- **Vibration Sensors**: Surface texture, slip detection

## Setting Up a Multi-Sensor Robot

### Robot Model with Integrated Sensors

Let's extend our basic humanoid model with various sensors. Here's an example SDF file with multiple sensor types:

```xml
<?xml version="1.0" ?>
<sdf version="1.7">
  <model name="sensor_humanoid">
    <pose>0 0 1 0 0 0</pose>

    <!-- Torso with main sensors -->
    <link name="torso">
      <inertial>
        <mass>10.0</mass>
        <inertia>
          <ixx>0.5</ixx>
          <iyy>0.5</iyy>
          <izz>0.5</izz>
          <ixy>0</ixy>
          <ixz>0</ixz>
          <iyz>0</iyz>
        </inertia>
      </inertial>
      <visual name="visual">
        <geometry>
          <box>
            <size>0.5 0.3 0.8</size>
          </box>
        </geometry>
        <material>
          <ambient>0.5 0.5 1 1</ambient>
          <diffuse>0.5 0.5 1 1</diffuse>
          <specular>0.5 0.5 1 1</specular>
        </material>
      </visual>
      <collision name="collision">
        <geometry>
          <box>
            <size>0.5 0.3 0.8</size>
          </box>
        </geometry>
      </collision>

      <!-- IMU sensor in torso -->
      <sensor name="torso_imu" type="imu">
        <always_on>true</always_on>
        <update_rate>100</update_rate>
        <imu>
          <angular_velocity>
            <x>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.001</stddev>
              </noise>
            </x>
            <y>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.001</stddev>
              </noise>
            </y>
            <z>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.001</stddev>
              </noise>
            </z>
          </angular_velocity>
          <linear_acceleration>
            <x>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.017</stddev>
              </noise>
            </x>
            <y>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.017</stddev>
              </noise>
            </y>
            <z>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.017</stddev>
              </noise>
            </z>
          </linear_acceleration>
        </imu>
      </sensor>

      <!-- 2D LiDAR on torso -->
      <sensor name="torso_lidar" type="ray">
        <pose>0.2 0 0.1 0 0 0</pose>
        <ray>
          <scan>
            <horizontal>
              <samples>360</samples>
              <resolution>1</resolution>
              <min_angle>-3.14159</min_angle>
              <max_angle>3.14159</max_angle>
            </horizontal>
          </scan>
          <range>
            <min>0.1</min>
            <max>10.0</max>
            <resolution>0.01</resolution>
          </range>
        </ray>
        <always_on>true</always_on>
        <update_rate>10</update_rate>
        <visualize>false</visualize>
      </sensor>
    </link>

    <!-- Head with vision sensors -->
    <link name="head">
      <pose>0 0 0.6 0 0 0</pose>
      <inertial>
        <mass>2.0</mass>
        <inertia>
          <ixx>0.02</ixx>
          <iyy>0.02</iyy>
          <izz>0.02</izz>
          <ixy>0</ixy>
          <ixz>0</ixz>
          <iyz>0</iyz>
        </inertia>
      </inertial>
      <visual name="visual">
        <geometry>
          <sphere>
            <radius>0.15</radius>
          </sphere>
        </geometry>
        <material>
          <ambient>0.8 0.8 0.8 1</ambient>
          <diffuse>0.8 0.8 0.8 1</diffuse>
          <specular>0.8 0.8 0.8 1</specular>
        </material>
      </visual>
      <collision name="collision">
        <geometry>
          <sphere>
            <radius>0.15</radius>
          </sphere>
        </geometry>
      </collision>

      <!-- RGB camera -->
      <sensor name="rgb_camera" type="camera">
        <pose>0.1 0 0 0 0 0</pose>
        <camera>
          <horizontal_fov>1.047</horizontal_fov>
          <image>
            <width>640</width>
            <height>480</height>
            <format>R8G8B8</format>
          </image>
          <clip>
            <near>0.1</near>
            <far>10</far>
          </clip>
        </camera>
        <always_on>true</always_on>
        <update_rate>30</update_rate>
        <visualize>true</visualize>
      </sensor>

      <!-- Depth camera -->
      <sensor name="depth_camera" type="depth">
        <pose>0.1 0.05 0 0 0 0</pose>
        <camera>
          <horizontal_fov>1.047</horizontal_fov>
          <image>
            <width>640</width>
            <height>480</height>
          </image>
          <clip>
            <near>0.1</near>
            <far>10</far>
          </clip>
        </camera>
        <always_on>true</always_on>
        <update_rate>30</update_rate>
        <visualize>false</visualize>
      </sensor>
    </link>

    <!-- Joint connecting head to torso -->
    <joint name="neck_joint" type="revolute">
      <parent>torso</parent>
      <child>head</child>
      <axis>
        <xyz>0 0 1</xyz>
        <limit>
          <lower>-0.5</lower>
          <upper>0.5</upper>
          <effort>100</effort>
          <velocity>1</velocity>
        </limit>
      </axis>
    </joint>

    <!-- Left hand with force/torque sensor -->
    <link name="left_hand">
      <pose>0.8 -0.2 0.3 0 0 0</pose>
      <inertial>
        <mass>0.5</mass>
        <inertia>
          <ixx>0.001</ixx>
          <iyy>0.001</iyy>
          <izz>0.001</izz>
          <ixy>0</ixy>
          <ixz>0</ixz>
          <iyz>0</iyz>
        </inertia>
      </inertial>
      <visual name="visual">
        <geometry>
          <box>
            <size>0.1 0.1 0.1</size>
          </box>
        </geometry>
        <material>
          <ambient>0.9 0.5 0.1 1</ambient>
          <diffuse>0.9 0.5 0.1 1</diffuse>
          <specular>0.9 0.5 0.1 1</specular>
        </material>
      </visual>
      <collision name="collision">
        <geometry>
          <box>
            <size>0.1 0.1 0.1</size>
          </box>
        </geometry>
      </collision>

      <!-- Force/Torque sensor -->
      <sensor name="left_hand_force_torque" type="force_torque">
        <always_on>true</always_on>
        <update_rate>100</update_rate>
        <force_torque>
          <frame>child</frame>
          <measure_direction>child_to_parent</measure_direction>
        </force_torque>
      </sensor>
    </link>

    <!-- Joint connecting left hand to left arm -->
    <joint name="left_gripper_joint" type="fixed">
      <parent>torso</parent>
      <child>left_hand</child>
    </joint>
  </model>
</sdf>
```

## Sensor Configuration and Parameters

### LiDAR Configuration

Configure LiDAR sensors for different use cases:

#### Navigation LiDAR
- **Range**: 10-30 meters for indoor navigation
- **Resolution**: 1cm for obstacle detection
- **Update Rate**: 10-20 Hz for navigation
- **Field of View**: 360째 horizontal, 10-30째 vertical

#### Mapping LiDAR
- **Range**: 50-100 meters for mapping
- **Resolution**: 1-5mm for detailed mapping
- **Update Rate**: 5-10 Hz for mapping
- **Field of View**: Variable based on mapping needs

### Camera Configuration

Configure cameras for different vision tasks:

#### Navigation Camera
- **Resolution**: 640x480 to 1280x720
- **Field of View**: 60-90째 for navigation
- **Update Rate**: 15-30 FPS for navigation
- **Format**: RGB8 for standard vision

#### Manipulation Camera
- **Resolution**: 1280x720 to 1920x1080
- **Field of View**: 30-60째 for detailed manipulation
- **Update Rate**: 30-60 FPS for real-time manipulation
- **Format**: RGBD for 3D manipulation

### IMU Configuration

Configure IMU sensors for different applications:

#### Balance Control IMU
- **Update Rate**: 100-200 Hz for balance control
- **Noise**: Low noise for stable balance
- **Bias**: Stable bias for accurate measurements
- **Range**: Appropriate for humanoid movements

#### Navigation IMU
- **Update Rate**: 50-100 Hz for navigation
- **Noise**: Moderate noise for realistic navigation
- **Bias**: Drifting bias for realistic long-term behavior
- **Range**: Sufficient for navigation tasks

## Data Synchronization

### Time Synchronization

Ensuring all sensors have synchronized timestamps:

```python
#!/usr/bin/env python3
"""
Example script for sensor synchronization
"""
import rospy
from sensor_msgs.msg import Imu, Image, LaserScan
from std_msgs.msg import Header
from threading import Lock

class SensorSynchronizer:
    def __init__(self):
        # Initialize ROS node
        rospy.init_node('sensor_synchronizer')

        # Create subscribers for different sensors
        rospy.Subscriber('/sensor_humanoid/rgb_camera/image_raw', Image, self.camera_callback)
        rospy.Subscriber('/sensor_humanoid/torso_lidar/scan', LaserScan, self.lidar_callback)
        rospy.Subscriber('/sensor_humanoid/torso_imu', Imu, self.imu_callback)

        # Publishers for synchronized data
        self.sync_pub = rospy.Publisher('/sensor_humanoid/synchronized_data', Header, queue_size=10)

        # Storage for sensor data with timestamps
        self.camera_data = None
        self.lidar_data = None
        self.imu_data = None

        # Lock for thread safety
        self.lock = Lock()

        # Synchronization parameters
        self.sync_window = rospy.Duration(0.01)  # 10ms window for synchronization

    def camera_callback(self, msg):
        with self.lock:
            self.camera_data = msg

    def lidar_callback(self, msg):
        with self.lock:
        self.lidar_data = msg

    def imu_callback(self, msg):
        with self.lock:
            self.imu_data = msg

    def check_synchronization(self):
        """Check if all sensors have recent data within sync window"""
        if not all([self.camera_data, self.lidar_data, self.imu_data]):
            return False

        now = rospy.Time.now()
        # Check if all data is within sync window
        return (now - self.camera_data.header.stamp < self.sync_window and
                now - self.lidar_data.header.stamp < self.sync_window and
                now - self.imu_data.header.stamp < self.sync_window)

    def publish_synchronized_data(self):
        """Publish synchronized data bundle"""
        if self.check_synchronization():
            # Create synchronized data message
            sync_msg = Header()
            sync_msg.stamp = rospy.Time.now()
            sync_msg.frame_id = "synchronized_bundle"

            # Publish the synchronized data
            self.sync_pub.publish(sync_msg)

            # Clear data to wait for next sync
            with self.lock:
                self.camera_data = None
                self.lidar_data = None
                self.imu_data = None

if __name__ == '__main__':
    synchronizer = SensorSynchronizer()

    rate = rospy.Rate(100)  # Check 100 times per second
    while not rospy.is_shutdown():
        synchronizer.publish_synchronized_data()
        rate.sleep()
```

### Coordinate System Alignment

Ensure all sensors use consistent coordinate systems:

#### ROS Standard (REP-103)
- **X**: Forward
- **Y**: Left
- **Z**: Up
- **Rotation**: Right-handed (roll, pitch, yaw)

#### Transform Management

Use TF (Transform) tree to maintain coordinate relationships:

```xml
<!-- In URDF/SDF, define transforms between sensors -->
<sensor name="rgb_camera" type="camera">
  <pose>0.1 0 0 0 0 0</pose> <!-- 10cm forward from head frame -->
  <!-- ... -->
</sensor>

<sensor name="depth_camera" type="depth">
  <pose>0.1 0.05 0 0 0 0</pose> <!-- 10cm forward, 5cm left from head frame -->
  <!-- ... -->
</sensor>
```

## Sensor Validation and Testing

### Data Quality Assessment

#### Range Sensor Validation
```bash
# Test LiDAR data quality
rostopic echo /sensor_humanoid/torso_lidar/scan --field=ranges | head -n 20

# Monitor camera data
rostopic hz /sensor_humanoid/rgb_camera/image_raw

# Check IMU data
rostopic echo /sensor_humanoid/torso_imu --field=orientation
```

#### Image Quality Testing
```bash
# View camera feed
rosrun image_view image_view image:=/sensor_humanoid/rgb_camera/image_raw

# Check depth image
rosrun image_view image_view image:=/sensor_humanoid/depth_camera/image_raw
```

### Performance Monitoring

Monitor sensor performance:

#### Bandwidth Requirements
- **RGB Camera (640x480@30Hz)**: ~27 MB/s
- **Depth Camera (640x480@30Hz)**: ~36 MB/s
- **LiDAR (360 samples@10Hz)**: ~1 KB/s
- **IMU (100Hz)**: ~10 KB/s

#### Processing Load
- **Camera Processing**: CPU/GPU intensive
- **LiDAR Processing**: Moderate CPU load
- **IMU Processing**: Low CPU load
- **Data Synchronization**: Moderate CPU load

## Multi-Sensor Fusion

### Basic Fusion Example

Create a simple sensor fusion node:

```python
#!/usr/bin/env python3
"""
Basic sensor fusion example combining camera and LiDAR data
"""
import rospy
import numpy as np
from sensor_msgs.msg import Image, LaserScan
from cv_bridge import CvBridge
import cv2

class SensorFusionNode:
    def __init__(self):
        rospy.init_node('sensor_fusion_node')

        # Initialize CvBridge for image processing
        self.bridge = CvBridge()

        # Subscribe to sensors
        rospy.Subscriber('/sensor_humanoid/rgb_camera/image_raw', Image, self.camera_callback)
        rospy.Subscriber('/sensor_humanoid/torso_lidar/scan', LaserScan, self.lidar_callback)

        # Publisher for fused data
        self.fused_pub = rospy.Publisher('/sensor_humanoid/fused_perception', Image, queue_size=10)

        # Storage for sensor data
        self.latest_camera = None
        self.latest_lidar = None

        # Camera processing parameters
        self.camera_processing_enabled = True

    def camera_callback(self, msg):
        self.latest_camera = msg

    def lidar_callback(self, msg):
        self.latest_lidar = msg

    def process_and_fuse(self):
        """Process and fuse sensor data"""
        if self.latest_camera is None or self.latest_lidar is None:
            return

        try:
            # Convert ROS image to OpenCV
            cv_image = self.bridge.imgmsg_to_cv2(self.latest_camera, "bgr8")

            # Process LiDAR data to overlay on image
            # This is a simplified example - in practice, you'd need proper calibration
            # and projection matrices to map LiDAR points to image space

            # Draw LiDAR data overlay (simplified visualization)
            if self.camera_processing_enabled:
                # Draw distance information as overlay
                height, width = cv_image.shape[:2]

                # Example: draw distance rings based on LiDAR data
                for i, distance in enumerate(self.latest_lidar.ranges):
                    if 0 < distance < 10:  # Valid range
                        # Map distance to image coordinates (simplified)
                        angle = self.latest_lidar.angle_min + i * self.latest_lidar.angle_increment
                        x = int(width/2 + distance * 20 * np.cos(angle))
                        y = int(height/2 + distance * 20 * np.sin(angle))

                        if 0 <= x < width and 0 <= y < height:
                            cv2.circle(cv_image, (x, y), 3, (0, 255, 0), -1)

            # Publish fused visualization
            fused_msg = self.bridge.cv2_to_imgmsg(cv_image, "bgr8")
            fused_msg.header = self.latest_camera.header
            self.fused_pub.publish(fused_msg)

        except Exception as e:
            rospy.logerr(f"Error in sensor fusion: {e}")

if __name__ == '__main__':
    fusion_node = SensorFusionNode()

    rate = rospy.Rate(10)  # 10 Hz fusion rate
    while not rospy.is_shutdown():
        fusion_node.process_and_fuse()
        rate.sleep()
```

## AI Perception Integration

### Synthetic Data Generation

Create synthetic data for AI training:

#### Data Collection Script
```python
#!/usr/bin/env python3
"""
Synthetic data collection for AI training
"""
import rospy
import cv2
import numpy as np
import os
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
from datetime import datetime

class SyntheticDataCollector:
    def __init__(self):
        rospy.init_node('synthetic_data_collector')

        # Setup directories
        self.base_dir = "/tmp/synthetic_robot_data"
        self.image_dir = os.path.join(self.base_dir, "images")
        self.depth_dir = os.path.join(self.base_dir, "depth")
        self.annotations_dir = os.path.join(self.base_dir, "annotations")

        for directory in [self.image_dir, self.depth_dir, self.annotations_dir]:
            os.makedirs(directory, exist_ok=True)

        # Initialize CvBridge
        self.bridge = CvBridge()

        # Subscribe to camera data
        rospy.Subscriber('/sensor_humanoid/rgb_camera/image_raw', Image, self.rgb_callback)
        rospy.Subscriber('/sensor_humanoid/depth_camera/image_raw', Image, self.depth_callback)

        # Data collection parameters
        self.collection_rate = rospy.Rate(5)  # 5 Hz
        self.collection_enabled = True

        # Counter for unique filenames
        self.data_counter = 0

    def rgb_callback(self, msg):
        if not self.collection_enabled:
            return

        try:
            # Convert to OpenCV image
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")

            # Save image with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            filename = f"rgb_{timestamp}_{self.data_counter:06d}.png"
            filepath = os.path.join(self.image_dir, filename)

            cv2.imwrite(filepath, cv_image)
            rospy.loginfo(f"Saved RGB image: {filename}")

            self.data_counter += 1

        except Exception as e:
            rospy.logerr(f"Error saving RGB image: {e}")

    def depth_callback(self, msg):
        if not self.collection_enabled:
            return

        try:
            # Convert to OpenCV image (depth data)
            cv_depth = self.bridge.imgmsg_to_cv2(msg, "32FC1")

            # Normalize depth for visualization
            depth_normalized = cv2.normalize(cv_depth, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

            # Save depth image
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            filename = f"depth_{timestamp}_{self.data_counter:06d}.png"
            filepath = os.path.join(self.depth_dir, filename)

            cv2.imwrite(filepath, depth_normalized)
            rospy.loginfo(f"Saved depth image: {filename}")

        except Exception as e:
            rospy.logerr(f"Error saving depth image: {e}")

if __name__ == '__main__':
    collector = SyntheticDataCollector()

    rospy.spin()
```

### Ground Truth Generation

Generate ground truth annotations for training:

```python
#!/usr/bin/env python3
"""
Ground truth generation for synthetic data
"""
import rospy
import numpy as np
import json
from gazebo_msgs.msg import ModelStates
from geometry_msgs.msg import Point

class GroundTruthGenerator:
    def __init__(self):
        rospy.init_node('ground_truth_generator')

        # Subscribe to Gazebo model states for ground truth
        rospy.Subscriber('/gazebo/model_states', ModelStates, self.model_states_callback)

        # Store model information
        self.model_states = None
        self.object_positions = {}

    def model_states_callback(self, msg):
        self.model_states = msg

        # Update object positions
        for i, name in enumerate(msg.name):
            if "box" in name or "cylinder" in name or "sphere" in name:
                self.object_positions[name] = {
                    'position': {
                        'x': msg.pose[i].position.x,
                        'y': msg.pose[i].position.y,
                        'z': msg.pose[i].position.z
                    },
                    'orientation': {
                        'x': msg.pose[i].orientation.x,
                        'y': msg.pose[i].orientation.y,
                        'z': msg.pose[i].orientation.z,
                        'w': msg.pose[i].orientation.w
                    }
                }

    def generate_annotations(self):
        """Generate JSON annotations for synthetic data"""
        if not self.object_positions:
            return None

        annotations = {
            'timestamp': rospy.Time.now().to_sec(),
            'objects': self.object_positions,
            'camera_info': {
                # Add camera intrinsic parameters
                'fx': 320.0,  # Focal length x
                'fy': 320.0,  # Focal length y
                'cx': 320.0,  # Principal point x
                'cy': 240.0,  # Principal point y
                'width': 640,
                'height': 480
            }
        }

        return annotations

if __name__ == '__main__':
    generator = GroundTruthGenerator()
    rate = rospy.Rate(10)  # 10 Hz annotation generation

    while not rospy.is_shutdown():
        annotations = generator.generate_annotations()
        if annotations:
            # In a real implementation, you would save this with the corresponding images
            print(f"Generated annotations for {len(annotations['objects'])} objects")
        rate.sleep()
```

## Troubleshooting Common Issues

### Sensor Synchronization Problems

**Problem**: Sensor data arrives at different times
**Solutions**:
- Use ROS message filters for time synchronization
- Implement custom synchronization with time windows
- Check simulation real-time factor settings
- Verify sensor update rates in configuration

### Data Quality Issues

**Problem**: Sensor data contains artifacts or noise
**Solutions**:
- Verify sensor configuration parameters
- Check for proper noise modeling
- Validate coordinate system alignment
- Test with simple, known environments

### Performance Bottlenecks

**Problem**: High computational load from multiple sensors
**Solutions**:
- Reduce sensor update rates where possible
- Use lower resolution for less critical sensors
- Implement efficient data processing pipelines
- Consider sensor-specific processing on separate nodes

### Integration Failures

**Problem**: Sensors don't appear in simulation or don't publish data
**Solutions**:
- Verify sensor plugin configuration
- Check Gazebo and ROS connection
- Validate SDF/URDF syntax
- Ensure proper topic names and namespaces

## Best Practices

### Sensor Placement

- **Minimize Occlusion**: Place sensors to avoid robot body blocking
- **Redundancy**: Use multiple sensors for critical functions
- **Clear Field of View**: Ensure sensors have unobstructed views
- **Protection**: Consider sensor placement for physical protection

### Data Management

- **Efficient Storage**: Use appropriate compression for sensor data
- **Bandwidth Optimization**: Balance quality with transmission requirements
- **Synchronization**: Maintain proper timing relationships
- **Calibration**: Regularly validate sensor calibration

### Validation Strategy

- **Individual Testing**: Test each sensor separately
- **Integration Testing**: Test sensor combinations
- **Real-World Comparison**: Validate against real sensors when possible
- **Edge Case Testing**: Test with challenging scenarios

## Summary

This tutorial has covered the complete workflow for sensor integration in Digital Twin environments:
- Setting up multi-sensor robot models with various sensor types
- Configuring sensors with appropriate parameters for different applications
- Implementing data synchronization for multi-sensor systems
- Validating sensor performance and data quality
- Creating sensor fusion systems for enhanced perception
- Generating synthetic data for AI training

The key to successful sensor integration is understanding the specific requirements of your application and configuring sensors appropriately. Always validate your sensor setup with both individual and combined testing to ensure reliable performance.

## Next Steps

After completing this tutorial, consider exploring:
- Advanced sensor fusion algorithms for complex perception tasks
- Integration with machine learning frameworks for perception
- Real-time optimization of sensor configurations
- Development of specialized sensors for specific applications
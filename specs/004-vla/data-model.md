# Data Model: Vision-Language-Action (VLA) Integration

## Core Entities

### VoiceCommand
- **Description**: Natural language input from users that needs to be processed and converted to robot actions
- **Attributes**:
  - id: Unique identifier for the command
  - text: Transcribed text from voice input
  - timestamp: When the command was received
  - confidence: Confidence score from speech recognition (0.0-1.0)
  - original_audio: Reference to original audio data (for debugging)
- **Validation**: Text must not be empty, confidence must be between 0.0 and 1.0

### ActionSequence
- **Description**: Series of ROS 2 actions generated from voice commands
- **Attributes**:
  - id: Unique identifier for the action sequence
  - steps: Array of individual action steps
  - robot_id: Identifier of the target robot
  - priority: Priority level (1-5)
  - estimated_duration: Estimated time to complete sequence
  - status: Current execution status (pending, executing, completed, failed)
- **State Transitions**: pending → executing → (completed | failed)

### CognitivePlan
- **Description**: High-level plan generated by LLMs that breaks down complex commands
- **Attributes**:
  - id: Unique identifier for the plan
  - original_command: Reference to the original VoiceCommand
  - plan_steps: Array of high-level plan steps
  - constraints: Environmental and safety constraints
  - dependencies: Order dependencies between steps
  - confidence: LLM's confidence in the plan (0.0-1.0)
- **Validation**: Must have at least one plan step, confidence must be between 0.0 and 1.0

### VLAExecutionContext
- **Description**: Execution context that tracks the state of the VLA system
- **Attributes**:
  - id: Unique identifier for the execution context
  - current_state: Current state of the humanoid robot
  - environment_data: Sensor data from the environment
  - execution_history: Log of executed actions
  - feedback_data: Results from completed actions
- **State Transitions**: idle → processing_voice → planning → executing → completed

## Relationships

- VoiceCommand → ActionSequence (one-to-many): One voice command can result in multiple action sequences
- CognitivePlan → ActionSequence (one-to-many): One cognitive plan can generate multiple action sequences
- ActionSequence → VLAExecutionContext (many-to-one): Multiple action sequences may be part of one execution context

## Data Flow

The data flows through the system as follows:
1. VoiceCommand is created from user voice input
2. CognitivePlan is generated from the VoiceCommand using LLM processing
3. ActionSequence is created from the CognitivePlan
4. VLAExecutionContext manages the execution of the ActionSequence